\documentclass{article}

\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{bbm}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{inconsolata}

\usepackage[sorting=none,backend=bibtex]{biblatex}
\bibliography{biblio}


\newcommand{\RR}{\mathbb{R}}
\newcommand{\Shift}{\mathcal{S}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\JJ}{\mathbb{J}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\VV}{\mathcal{V}}
\newcommand{\MM}{\mathcal{M}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\uu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\CC}{C\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf +}}
\def\CC{{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}}
\newcommand{\dP}{\mathcal{P}}
% operator odvoda
\newcommand{\D}{\partial}
%operator 1 + \D
\newcommand{\DD}{\mathcal{D}}
% operator 1+ \D + \D^2 + ...
\newcommand{\sumd}{\tau}
\newcommand{\Op}{\partial^{\bigoplus}}
\newcommand{\op}[1]{\partial^{#1\bigoplus}}
\DeclareMathOperator{\interior}{int}

\newcommand{\dCpp}{dC\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf p}}
\def\dCpp{{dC\nolinebreak[4]\hspace{-.05em}pp}}

\usepackage{listings}
\usepackage{xcolor}
\lstset { %
    language=C++,
    backgroundcolor=\color{black!5}, % set backgroundcolor
    basicstyle=\footnotesize,% basic font setting
}

\newtheorem{definicija}{Definition}[section]
\newtheorem{trditev}{Claim}[section]
\newtheorem{izrek}{Theorem}[section]
\newtheorem{opomba}{Remark}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]

\usepackage[hidelinks]{hyperref}
\usepackage{epigraph}
\usepackage{etoolbox}

\makeatletter
\patchcmd{\epigraph}{\@epitext{#1}}{\itshape\@epitext{#1}}{}{}
\makeatother

\usepackage{tikz}
\usepackage{tikz-cd}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{positioning}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{pifont}

%\title{Analytic virtual machine\protect \\--- \protect \\ Implementation and Employment}
\title{Implementing \\ Operational calculus on programming spaces \\ for Differentiable computing}
\author{Žiga Sajovic\footnote{University of Ljubljana, Faculty of Computer and Information
  Science,  Večna pot 113, SI-1000 Ljubljana, Slovenia}\footnote{XLAB d.o.o., Pot za Brdom 100, SI-1000 Ljubljana, Slovenia\newline \href{mail.to:ziga.sajovic@xlab.si}{ziga.sajovic@gmail.com}}}


\begin{document}

\maketitle

\begin{abstract}
We provide an illustrative implementation of an analytic, infinitely-differentiable virtual machine, implementing infinitely-differentiable programming spaces and operators acting upon them, as constructed in the paper \emph{Operational calculus on programming spaces and generalized tensor networks}\cite{OperationalCalculus}. Implementation closely follows theorems and derivations of the paper, intended as an educational guide for those transitioning from automatic differentiation to this general theory.

\end{abstract}

\epigraph{Nothing at all takes place in the universe in which some rule of maximum or minimum does not appear.}{--- \textup{Leonhard Euler}}

\clearpage

\tableofcontents
\clearpage
\section{Introduction}\label{sec:introduction}
We provide an illustrative implementation of the virtual memory $\VV$ and its expansion to $\VV\otimes T(\VV^*)$, serving by itself as an algebra of programs along with an infinitely differentiable programming space \cite[Theorem~14]{OperationalCalculus}
\begin{equation}
\DD^n\CC<\dP_n:\VV\to\VV\otimes T(\VV^*)
\end{equation}
acting on it.

We provide a construction of operators
\begin{equation}
\DD^n=\{\D^k;\quad 0\le k\le n\}
\end{equation}
allowing derivation of the generalized shift operator \cite[Theorem~17]{OperationalCalculus}
 \begin{equation}
                  	e^{h\D}:\dP\times \VV\to \VV\otimes \T(\VV^*),
         \end{equation}
and projecting it onto the unit $n$-cube, deriving the operator increasing the order of a differentiable programming space \cite[Proposition~11]{OperationalCalculus}
\begin{equation}\label{eq:sumd}
\sumd_n:\dP_k\to\dP_{n+k}
\end{equation} 

Analytic virtual machines allow nested transformations, with operational calculus opening new doors in program analysis. We outline the process of employing such a machine to several causes, from engineering to image processing, seamlessly interweaving operational calculus and algorithmic control flow. Source code can be found on GitHub \cite{dCpp}.

\section{Virtual memory}\label{sec:virtualMemory}

We model an element of the virtual memory $v\in\VV_n$ \cite[Definition~7]{OperationalCalculus}
\begin{eqnarray}
\VV_{n}=\VV_{n-1}\oplus(V_{n-1}\otimes\VV^*) \label{eq:V_n}
\end{eqnarray}
\begin{equation}
\VV_{n}=\VV\oplus\VV\otimes\VV^*\oplus\cdots\oplus\VV\otimes\VV^{*n\otimes} \implies\VV_n=\VV\otimes T(\VV^*)
\end{equation}
with the class $var$.

\begin{lstlisting}
template<class V>
class var
{
    public:
    	int order;
        V id;
        std::shared_ptr<std::map<var*,var> >* dTau;

        var();
        var(V id);
        var(const var& other);
        ~var();
        void init(int order);
        var d(var* dVar);
        //declerations of algebraic operations
        //declerations of order logic
};
\end{lstlisting}

The expanded virtual memory $\VV_n$ is the tensor product of the virtual memory $\VV$ with the tensor algebra of its dual. Tensor products of the virtual memory $\VV$ with its dual are modeled using maps, denoted by \emph{dTau}. Naming reflects how the algebra is constructed, mimicking the operator $\sumd_n$ \eqref{eq:sumd}. The address \emph{var*} stands for the component of $v\in\VV_{n-1}$ on which the tensor product with the component of $v^*\in\VV^*$ was computed to generate $v\in\VV_n$ in equation \eqref{eq:V_n}. This depth is contained in the \emph{int order}.

\subsection{Initialization}

A constant element $v_0$ of the virtual memory is an element of $\VV_0=\VV<\VV_n$. We initialize an element to be $n$-differentiable, by mapping
\begin{equation}
init:\VV\times\mathbb{N}\to\VV_n
\end{equation}
\begin{equation}
v_0.init(n)=v_n\in\VV_n
\end{equation}
The image $v_n$ is an element of $\VV_1=\VV\otimes\VV^*$ with natural inclusion in $\VV_n$.
\begin{equation}
v_n=(v_0\in\VV)+(\delta^i_j\in\VV\otimes\VV^ {*\otimes})+\sum\limits_{i=2}^n(0\in\VV\otimes\VV^ {*i\otimes})
\end{equation}
where $\delta^i_j$ is the identity.
\subsection{Algebra over a field}\label{sec:Algebra}

 Algebra over a field is a vector space equipped with a bilinear product. Thus, an algebra is an algebraic structure, which consists of a set, together with operations of multiplication, addition, and scalar multiplication by elements of the underlying field. \cite[p.~3]{Algebra}
 
 Algebra is constructed by mimicking application of a direct sum of the operators $\sumd_n$ \eqref{eq:sumd} mapping $\dP_0\oplus\dP_0\to\dP_n$ to the maps of scalar multiplication, addition and a bilinear product. This is reflected in the structure of the class $var$  modeling the elements of the virtual space $v\in\VV_n$. 
 
 \begin{izrek}
 An instance of the class $var$ is an element of the virtual memory $\VV_n$.
  \begin{equation}
  var\in\VV_n
  \end{equation}
 \end{izrek}
 \begin{proof}
 \begin{equation}
  id\in \VV\land  dTau\in \VV_{n-1}
  \end{equation}
  $$\land$$
  \begin{equation}
  var=id\oplus dTau\implies var\in\VV_n
  \end{equation}
 \end{proof}
 
 Compositions are modeled by mimicking the operator of program composition \cite[Theorem~21]{OperationalCalculus}
 \begin{equation}\label{eq:kom}
   \exp(\D_fe^{h\D_g}): \dP\to\dP_\infty
   \end{equation}
 generalizing both forward and reverse mode automatic differentiation \cite[Remark~22]{OperationalCalculus}. By fixing one of the mappings in \eqref{eq:kom}, we derive projections of the pullback operator
 \begin{equation}\label{eq:komp}
    \exp(\D_fe^{h\D_g})(\cdot,g): \dP\to\dP_\infty(g)
    \end{equation}

  to the unit hyper-cube and applying it to the resulting direct sum.

\subsubsection{Vector space over a field $K$}\label{sec:vectorSpace}
 
We begin our construction of an algebra, by constructing a vector space $\VV_n$ over a field $K$. A vector space is collection of objects that can be added together and multiplied with elements of the underlying field $K$, .

We begin with scalar multiplication.

\begin{lstlisting}
template<class K>
var var::operator*(K n)const{
    var out;
    out.id=this->id*n;
    for_each_copy(...,mul_make_pair<pair<var*,var> >, n);
    return out;
}

template<class K>
var var::operator/(K n)const {...};
\end{lstlisting}
Scalar multiplication and its convenient inverse employ the function \emph{for\_each\_copy}, applying the provided operation 
\begin{lstlisting}
template<class V, class K>
V mul_make_pair(V v, K n) {
    return std::make_pair(v.first, v.second * n);
}
\end{lstlisting}
to each one of the components of $this$ and storing the result in $out.dTau$.

Vector addition by component is implemented by 
\begin{lstlisting}
var var::operator+(const var& v)const{
    var out;
    out.id=this->id+v.id;
    merge_apply(..., sum_pairs<pair<$var, var> >);
    return out;
}
\end{lstlisting}
Vector addition by component employs the function \emph{merge\_apply}, applying the provided function \emph{sum\_pairs}
\begin{lstlisting}
template<class V>
T sum_pairs(V v1, V v2) {
    return std::make_pair(v1.first, v1.second + v2.second);
}
\end{lstlisting}
to corresponding components, storing the result in $out.dTau$, in $\mathcal{O}(n\log(n))$.

\begin{izrek}
Class $var$ models a vector space over a field $K$.
\end{izrek}
\begin{proof}
By implementations of addition by components and multiplication with a scalar $k\in K$, the axioms of the vector space are satisfied.
\end{proof}

\subsubsection{Algebra over a field $K$}\label{sec:algebra}

With the vector space constructed, we turn towards its elevation to an algebra. To construct an algebra over a field $K$, we equip the vector space $\VV_n$ with a bilinear product by components.

\begin{lstlisting}
var var::operator*(const var& v)const{
    var out;
    out.id=this->real*v.id;
    out.order=this->order<v.order?this->order:v.order;
    if(out.order>0){
        map<int,double> tmp1;
        map<int,double> tmp2;
        for_each_copy(...,mul_make_pair<pair<var*,var> >, 
          v.reduce());
        for_each_copy(...,mul_make_pair<pair<var*,var> >, 
          this->reduce());
        merge_apply(..., sum_pairs<pair<var*,var> >);
    }
    return out;
}
\end{lstlisting}
The employed functions have been explained at previously usage. The \emph{reduce} function makes a shallow copy of $this$, while reducing the order of the returned copy. This bilinear product contains Leibniz rule within its structure, as the projection of the operator $\exp(\D_fe^{h\D_g})(g): \dP\to\dP_\infty(g)$ \eqref{eq:komp} to the unit $n$-cube was applied to the algebra.

\begin{izrek}
Class $var$ models an algebra over a field $K$.
\end{izrek}
\begin{proof}
By the implementation of a bilinear product by components the axioms of an algebra over a field are satisfied.
\end{proof}

For ease of expression we implement exponentiation, naturally existing in the algebra

\begin{lstlisting}
var var::operator^(double n) const{
    var out;
    out.id=std::pow(this->real,n);
    out.order=this->order<v.order?this->order:v.order;
    if(n>0&&out.order>0){
        for_each_copy(...,mul_make_pair<pair<var*,var> >, 
          (this->reduce()^(n-1))*=n);
    }
    return out;
}
\end{lstlisting}
employing the function \emph{for\_each\_copy}, applying the provided operation to each component.

We may now trivially implement operators existing in the algebra.

\begin{lstlisting}
var var::operator-(const var& v)const{
    return *this+(-1)*v;
}

var var::operator/(const var& v)const{
    return *this*(v^(-1));
}
\end{lstlisting}

Similarly, the following operators can be assumed to be generated by the existing algebra, implementation of which is omitted here for brevity.

\begin{lstlisting}
var operator*(double n)const;
var operator+(double n)const;
var operator-(double n)const;
var operator/(double n)const;
var operator^(double n) const;
var& operator=(const var& v);
var& operator+=(const var& v);
var& operator-=(const var& v);
var& operator*=(const var& v);
var& operator/=(const var& v);
var& operator*=(double n);
var& operator/=(double n);
var& operator+=(double n);
var& operator-=(double n);
var operator*(double n, const var& v);
var operator+(double n, const var& v);
var operator-(double n, const var& v);
var operator/(double n, const var& v);
var operator^(double n, const var& v);
\end{lstlisting} 

Order logic is trivially implemented, by mapping
\begin{equation}
\VV.id\times\VV.id\to\{0,1\}
\end{equation}
\begin{lstlisting}
bool operator==(const var& v)const;
bool operator!=(const var& v)const;
bool operator<(const var& v)const;
bool operator<=(const var& v)const;
bool operator>(const var& v)const;
bool operator>=(const var& v)const;
\end{lstlisting}

\section{Analytic virtual machine}\label{sec:analyticVmachine}

\begin{definicija}[Analytic virtual machine]
The tuple $M=\langle \VV,\dP_0\rangle$ is an analytic, infinitely  differentiable virtual machine.
   
    \begin{itemize}
    \item
    $\VV$ is a finite dimensional vector space
    \item
    $\VV\otimes T(\VV^*)$ is the virtual memory space, serving as alphabet symbols
    \item
    $\dP_0$ is an analytic programming space over $\VV$
    \end{itemize}
    When $\dP_0$ is a differentiable programming space, this defines an
    infinitely differentiable virtual machine. \cite[Defintion~15]{OperationalCalculus}
  \end{definicija}

  \subsection{Operators}\label{sec:operators}
        
        The operator of tensor series expansion \cite[Theorem~17]{OperationalCalculus}
        
        \begin{equation}\label{eq:specProg}
                  	e^{h\D}:\dP\times \VV\to \VV\otimes \T(\VV^*),
                  \end{equation}
        
        \begin{equation}
         	e^{h\D}=\sum\limits_{n=0}^{\infty}\frac{(h\D)^n}{n!}
         \end{equation}
         is evaluated at $h=1$ and projected onto the unit N-cube, arriving at 
        \begin{equation}\label{eq:DD}
          	\sumd_N = 1+\D +\D^2 +\ldots + \D^N,
          \end{equation}
        which is used to implement $\DD^n\CC\subset\dP_n$, employing the recursive relation for increasing the order of $\CC$ \cite[Proposition~11]{OperationalCalculus}.
        \begin{equation}
              \label{eq:rekurzija}
              \sumd_{k+1} = 1+\D\sumd_k,
            \end{equation}
        \begin{equation}
        \sumd_{k+1}\CC = \CC+\D\sumd_k\CC.
        \end{equation}
        
        The \emph{double id} stands for the identity operator $1$ in the expression and the \emph{map<var*, var> dTau} for the operator $\D\sumd_k$.
        
        Compositions of these operators is achieved by mimicking the generalized pullback operator \cite[Remark~11]{OperationalCalculus}
        \begin{equation}\label{eq:opKompo}
          \exp(\D_fe^{h\D_g})(g): \dP\to\dP_\infty(g),
          \end{equation}
          projected onto the unit N-cube.
          
          \begin{opomba}
          Implementation in this paper is intended to be simply understood and is written as such. But, with the existing algebra and operational calculus, one could easily implement the operator $\exp(\D_fe^{h\D_g})(g)$ by any of the efficient techniques available, as it is given by a generating function.
          \end{opomba}
        
\begin{lstlisting}
template<class dTau, class K>
class tau
{
public:
    tau();
    tau(K mapping, dTau primitive);
    ~tau();
    var operator()(const var&v);
private:
    dTau primitive;
    K mapping;
};
\end{lstlisting}        
        
\begin{lstlisting}
var tau::operator()(const var&v){
    var out;
    out.id=mapping(v.id);
    for_each_copy(...,mul_make_pair<std::pair<var*,var> >, 
      primitive(v.reduce()));
    return out;
}
\end{lstlisting}
        
\subsection{Differentiable programming space}\label{sec:differentiableProgSpace}
              
              With the algebra over $\VV_n$ implemented, we turn to the construction of a differentiable programming space
              
              \begin{equation}
              \CC:\VV\to\VV
              \end{equation}
              
              \begin{definicija}\label{def:dP}
               	A \emph{differentiable programming space} $\dP_0$ is any subspace of $\F_0$ such that
               	\begin{equation}\label{eq:P}
               	\D\dP_0\subset\dP_0\otimes T(V^*)
               	\end{equation}
               	When all elements of $\dP_0$ are analytic, we denote $\dP_0$ as an \emph{analytic programming space}. \cite[Definition~8]{OperationalCalculus}
               \end{definicija}
              
              \begin{izrek}\label{izr:P}
              	Any differentiable programming space $\dP_0$ is an
                infinitely differentiable programming space, such that
              	\begin{equation}\label{eq:P_n}
              	 		\D^k\dP_0\subset\dP_0\otimes T(V^*)
              	 	\end{equation}
              for any $k\in\mathbb{N}$. \cite[Theorem~9]{OperationalCalculus}
              \end{izrek}
              
              Thus, in order to have a differential programming space, we must provide closure under the differential operator \cite[Corollary~10]{OperationalCalculus}, for the function space $\CC$, expanded by the tensor product with the tensor algebra of the dual of the virtual memory $\VV$.
              
              \begin{equation}
              \DD^n\CC<\dP_n\iff\DD^n\CC\subset\CC\otimes T(\VV^*)
              \end{equation}

\begin{trditev}\label{trd:library}
Any library implementing functions acting on variables of type $double$ or $float$, could be trivially included into $\CC<\dP_0$, simply by replacing all variables with the class $var$. Moreover, any implementations using the implemented algebra in its construction of functions, are contained in $\dP_0$.
\end{trditev}

\subsubsection{Example}\label{sec:example}
  
As an illustrative example, we provide a simple implementation of a differentiable programming space $\dCpp$ through the use of the operator $tau$. 
Assume the existence of functions
\begin{equation}
\sin\_double:double\to double
\end{equation}
\begin{equation}
\cos\_double:double\to double
\end{equation} 
\begin{equation}
e\_double:double\to double
\end{equation}  
\begin{equation}
\ln\_double:double\to double
\end{equation} 
filling the set spanning $\dCpp<\CC$. Note, that these functions are usually implemented using operations existing in the algebra on $\VV_n$ constructed in Section \ref{sec:Algebra}. Thus by employing it in their construction (coding), they would have been elements of a differentiable programming space, as by Claim \ref{trd:library}. Here we demonstrate how to explicitly construct them as maps.

Previous declarations of needed functions are assumed. 

\begin{lstlisting}
namespace dCpp{
    var sin(const var& v);
    tau cos;
    tau e;
    tau ln;
    var cos_primitive(const var& v);
    var ln_primitive(const var& v);
    var e_primitive(const var& v);
}
\end{lstlisting}

We construct the map $sin$ explicitly for educational purposes

\begin{lstlisting}
var dC::sin(const var& v){
    var out;
    out.id=std::sin_double(v.id);
    out.order=v.order;
    if(v.order>0){
    	for_each_copy(...,mul_make_pair<std::pair<var*,var> >,
    		cos(v.reduce()));
    }
    return out;
}
\end{lstlisting}

Other maps are constructed through employment of the operator $tau$.


\begin{lstlisting}
typedef var (*dTau)(var);
typedef double (*mapping)(double);

var dCpp::cos_primitive(cont $var v){
    return (-1)*sin(v);
}

dCpp::cos=tau<dTau,mapping>(cos_double,cos_primitive);

var dCpp::ln_primitive(cont $var v){
    return 1/v;
}

dCpp::ln=tau<dTau,mapping>(ln_double,ln_primitive);

dCpp::e_primitive(cont $var v){
    return e(v);
}

tau dCpp::e=tau<dTau,mapping>(e_double,e_primitive);

\end{lstlisting}

\begin{izrek}\label{izr:dCpp}
The programming space $\dCpp$ is a differentiable programming space satisfying
\begin{equation}
\dCpp<\dP_0\iff \DD \dCpp\subset \dCpp\otimes T(\VV^*)
\end{equation}
\end{izrek}
\begin{proof}
\begin{equation}
\forall_{\Phi_i\in \dCpp}\exists_{\Phi_j\in \dCpp}(\Phi_i.primitive=\Phi_j)
\end{equation}
$$\implies$$
\begin{equation}
\DD \dCpp\subset \dCpp\otimes T(\VV^*)
\end{equation}
\end{proof}
\begin{corollary}
The programming space $\dCpp$ is an infinitely-differentiable programming space satisfying
\begin{equation}
\dCpp<\dP_0\iff \DD^n \dCpp\subset \dCpp\otimes T(\VV^*)
\end{equation}
\end{corollary}
\begin{proof}
Follows directly from Theorem \ref{izr:dCpp} by Theorem \ref{izr:P}.
\end{proof}

\begin{corollary}
The tuple 
\begin{equation}
M=\langle\VV,\dCpp\rangle
\end{equation}
is an Analytic virtual machine.
\end{corollary}

\subsubsection{Order reduction for nested applications}\label{sec:orderReduction}
 
 It is useful to be able to use the $k$-th derivative of a program $P\in\dP$ as part of a different differentiable program $P_1$. As such, we must be able to treat the derivative itself as a differentiable program $P^{\prime k}\in\dP$, while only coding the original program $P$. \cite[Section~5.3]{OperationalCalculus}
\begin{izrek}\label{izr:reductionMap}
There exists a reduction of order map $\phi:\dP_n\to \dP_{n-1}$, such that the
following  diagram commutes
\begin{equation}\label{eq:reductionMap}
\begin{tikzcd}
  \dP_n \arrow{r}{\phi} \arrow{d}{\D} & 
  \dP_{n-1} \arrow{d}{\D}\\
  \dP_{n+1} \arrow{r}{\phi} & 
  \dP_{n}
\end{tikzcd}
\end{equation}
satisfying
\begin{equation}
\forall_{P_1\in\dP_0}\exists_{P_2\in\dP_0}\Big(\phi^k\circ \sumd_n(P_1)=\sumd_{n-k}(P_2)\Big)
\end{equation}
for each $n\ge 1$.
\end{izrek}  
\begin{corollary}\label{cor:extraxtDerivatives}
By Theorem \ref{izr:reductionMap}, $n$-differentiable $k$-th derivatives of a program $P\in\dP_0$ can be extracted by
\begin{equation}
^{n}P^{k\prime}=\phi^k\circ \sumd_{n+k}(P)\in\dP_n
\end{equation}
\end{corollary}  
\begin{opomba}
Theorem \ref{izr:reductionMap} and Corollary \ref{cor:extraxtDerivatives} are derived by using projections onto the unit hyper-cube on \cite[Theorem~26]{OperationalCalculus} and \cite[Corollary~27]{OperationalCalculus}.
\end{opomba}  

We construct a reduction of order map $d:\dP_n\times\VV\to\dP_{n-1}$, 

\begin{lstlisting}
var var::d(var* dvar){
    return (*this->dTau.get())[dvar];
}
\end{lstlisting}
returning $n-1$ differentiable derivative of $v$ with respect to $v_i$.
Explicitly expressing $v\in\VV_n$ in terms of $v.d(\&v_i)$
\begin{equation}
v=v.id+\sum\limits_{\forall_i}v.d(\&v_i)\otimes dv_i \in\VV_n\implies v.d(\&v_i)\in\VV_{n-1}
\end{equation}
reveals the nature of the reduction of order map.

 Thus, we gained the ability of writing a differentiable program acting on derivatives of another program, stressed as crucial (but lacking in most models) by other authors \cite{AD1}. Usage of the reduction of order map and other constructs of
 this Section are demonstrated in Section \ref{sec:Employment}, as we analyze procedures as systems inducing change upon objects in virtual space. 

\subsection{External libraries}\label{sec:external}

Any $\CC$ library written in the generic paradigm employing templates is fully compatible with the differentiable programming space $\dCpp$ acting on the virtual memory $\VV$. 

We illustrate on the example of Eigen \cite{Eigen}. We will code a perceptron with sigmoid activations, followed by softmax normalization, taking 28x28 image as an input and outputting a 10 class classifier. Existence of needed, but trivial and intuitively understood mappings is assumed. 

\begin{lstlisting}
template <typename Derived>
    void softmax(Eigen::MatrixBase<Derived>& matrix){
            //maps each element of the matrix by y=e^x;
            dCpp::map_by_element(matrix,&dCpp::e);
            //sums the elements of the matrix using Eigens function
            var tmp=matrix.sum();
            //divides each element by the sum
            for (size_t i=0, nRows=matrix.rows(), 
            	nCols=matrix.cols(); i<nCols; ++i)
                for (size_t j=0; j<nRows; ++j){
                    matrix(j,i)/=tmp;
                }
}

int main(){
//order of derivatives needed
int order=...;
//    Matrix holding the inputs (imgSizeX1 vector)
const int imgSize=28*28;
const Eigen::Matrix<var,1,imgSize>input=Eigen::Matrix<var,1,
	imgSize>::Random(1,imgSize);
const int numOfOutOnFirstLevel=10;
//    matrix of weights on the first level 
//    (imgSizeXnumOfOutOnFirstLevel)
Eigen::Matrix<var,imgSize,numOfOutOnFirstLevel>firstLayerVars=
	Eigen::Matrix<var,imgSize,numOfOutOnFirstLevel>::
	Random(imgSize,numOfOutOnFirstLevel);
//    initializing weights
dCpp::init(firstLayerVars, order);
//    mapping of the first layer --> resulting in 10x1 vector
Eigen::Matrix<var,numOfOutOnFirstLevel,1>firstLayerOutput=
	input*firstLayerVars;
//    apply sigmoid layer --> resulting in 10x1 vector
dCpp::map_by_element(firstLayerOutput,&dCpp::sigmoid);
//    apply sofmax layer --> resulting in 10x1 vector
softmax(firstLayerOutput);
//retrieve the computed derivatives
\end{lstlisting}

\section{Applications}\label{sec:Employment}

The algebra over programs \cite[Theorem~14]{OperationalCalculus} an Analytic virtual machine enables and its operational calculus are employed towards achieving analytic conclusions through algebraic means, seamlessly interweaving operational calculus and algorithmic control flow. 

\subsection{General engineering}\label{sec:generalEngineering}

We demonstrate employment on the case of highway and railway design, where the author has previously used the described approach. Demonstration is presented in an easily transfered general form.

Highway and railway designers use clothoid splines (planar G1
curves consisting of straight line segments, circular arcs, and clothoid segments)
as center lines in route location \cite{ClothoidSpline}. These curves are usually computed algorithmically, using programs $P\in\dP_0$ and are used in other programs to compute various quantities. We focus on this inclusion.

Assume existence of a program mapping parameters $p_i$ and a point $x_i$ to a point $y_i$ on a clothoid.
\begin{equation}
cloth:p_i\times x_i\to y_i
\end{equation} 

Connecting two curves $\gamma_i$ via a clothoid, matching derivatives at endpoints is a solved problem \cite[Theorem~2]{ClothoidSpline}. In practice, algorithms solving it use derivatives of $cloth$ in its body. We provide it in terms of our implementation for clarity and use in the coming section. For simplicity, all parameters of $cloth$ are denoted by $p_i$, with $step$ being the employed optimization step using $k$-th derivatives in its body.

 \begin{algorithm}[H]
   \caption{Train}
   \label{alg:train}
   \begin{algorithmic}[1]
   \Procedure{Train}{n,k}
   \State initialize $P_i=var(p_i.id)\in\VV_{n}$
   \For{each step}
   \State re-initalize $p_i=var(P_i.id)\in\VV_{n+k}$
   \State extract derivatives $\D^k C_{p_i}=cloth(p_i).d^k(\&p_i)\in\dP_{n}$
   \State update parameters $step(\D^k C_{p_i}, P_i)\in\dP_n$
   \EndFor
   \State return $P_i\in\VV_{n}$
   \EndProcedure
   \end{algorithmic}
   \end{algorithm}
\begin{opomba}\label{rmr:dDerivatives}
Note that the derivatives $\D^k C_{p_i}$ are $n$-differentiable as by Corollary \ref{cor:extraxtDerivatives}, satisfying 
\begin{equation}
P_i\in\VV_n\iff \D^k C_{p_i}\in\dP_n
\end{equation}
This will hold meaning in the coming section. The $p_i$ denote all parameters, including the end-points. 
\end{opomba} 

\subsubsection{Training as a differentiable program}

The output of Algorithm \ref{alg:train} are the required parameters $P_i$, with the algorithm itself being an element of $\dP_{n}:\VV\to\VV\otimes T_{n}(\VV^*)$, making it a $n$-differentiable procedure. This becomes useful, as usually the output of Algorithm \ref{alg:train} (connecting many sections) is only a part of an algorithm evaluating the resulting spline. Optimizing this evaluation is the main desire.

We assume the end-points to be allowed to vary over a limited domain, and are determined by a program mapping some subset of the parameters $p_i$ (usually current end-points) of the curves forming the spline, to the set of adjusted end-points.
\begin{equation}
endP:\{p_i\}\to\{p_i\}
\end{equation}
Simulation of the traversal of the resulting spline is used to accumulate its evaluation.
\begin{equation}
eval:\{p_i\}\to K
\end{equation}
The desire is maximizing the evaluation, using $step$ as an optimization step using $n$-th derivatives in its body.

 \begin{algorithm}[H]
   \caption{Increase}
   \label{alg:Increase}
   \begin{algorithmic}[1]
   \Procedure{Increase}{n,k}
   \State initialize $p_i\in\VV_{n+k}$
   \For{each step}
   \State update end-points $endP(p_i)$
   \State update $p_i=Train(n+k,k)\in\dP_{n}$ (Algorithm \ref{alg:train})
   \State extract derivatives $\D^n C_{p_i}=eval(p_i).d^n(\&p_i)\in\dP_{0}$;
   \State update $step(\D^n C_{p_i},p_i)\in\dP_0$
   \EndFor
   \EndProcedure
   \end{algorithmic}
   \end{algorithm}
   \begin{opomba}
   Note that the derivatives $\D^n C_{p_i}$ are not differentiable, as they need not to be, as opposed to the derivatives used in Algorithm \ref{alg:train}, as by Remark \ref{rmr:dDerivatives}.
   \end{opomba}
Both algorithms can be generalized to other problems and were written as such. Mappings are trivially replaced, as are the parameters, due to the nature of their construction.

\subsection{Image processing}

Variational calculus has been increasingly present in digital image processing \cite{imageVariational},  most notoriously by geodesic active contours \cite{Caselles1997} and manifold flows, where the image is computed as a stationary map of an energy functional

\begin{equation}
E(I)=\int_{\mathbf{x}\in\Omega}g\left(I\left(\mathbf{x}\right)\right)d\mathbf{x}.
\end{equation}

  The mapping $g$ is usually some local mapping of on the image. 
  
  \subsubsection{Non-local methods}
  
  Recent results in inpainting show great success by employing non-local sampling in the functional \cite{inpaint}. 
  
  \begin{equation}\label{eq:nonL}
  E(I,w)=\int_{\tilde{O}}\int_{\tilde{O}^c}w(x,x'),\phi\left(||p_{I}(x)-p_{\tilde{I}}(x')||\right)dx'dx
  \end{equation}
  The weight function $w:O\times O^c\to\RR+$ measures similarity between patches centered in the in-painting domain and its compliment, and $\phi$ is the patch norm-like function \cite{inpaint} (usually containing an intra-patch weight function) \cite{inpaint}.
  
  Yet the methods are limited in their expressive power, due to the symbolic nature of their construction. Much more elaborate procedures of sampling could be constructed with the aid of algorithmic control flow.  
  As such, the subject is readily enriched by operational calculus on programming spaces. As a simple demonstration, we will adapt the usual model for geodesic active contours, to include non-local methods similar to \eqref{eq:nonL}.
  
  The standard functional is
  \begin{equation}
  E(\mathbf{x})=\int_{C}g\left(\nabla I\left(\mathbf{x}\right)\right)ds.
  \end{equation}
  Yielding the flow
  \begin{equation}\label{eq:flow}
  \frac{\partial}{\partial t}C=g(\nabla I)\kappa \vec{N}-\langle\nabla g,\vec{N}\rangle\vec{N}
  \end{equation}
  We introduce non-locality to the method by redefining the edge indicator function $g$. 
  
   \begin{algorithm}[H]
     \caption{Non-Local indicator}
     \label{alg:nonL}
     \begin{algorithmic}[1]
     \Procedure{NonLocalIndicator}{$\mathbf{x}, \nabla img\in\VV_0$}
     \State initialize $accum\in\VV_1$
     \State initialize $\nabla I=var(\nabla img)\in\VV_1$
     \State $p_i=$getSimilarPatches$(\nabla I,\mathbf{x})\in\dP_1$
     \For{each patch $p_i$}
     \State current=x
     \For{iter}
     \State $accum+=g(|\nabla p_i(current)|)\in\dP_1$
     \State update current by randWalk
     \EndFor
     \EndFor
     \State return $accum\in\VV_1$
     \EndProcedure
     \end{algorithmic}
     \end{algorithm}
     
     \begin{opomba}
          Algorithm \ref{alg:nonL} can be trivially replaced by a more elaborate procedure $P\in\dP_1$.
          \end{opomba}
   
   We tailor  our intervention, to be as non invasive as possible to the standard method. As the flow \eqref{eq:flow} requires only the gradient of $g$ (as opposed to requiring the total derivative) we initialize $\nabla I\in\VV_1$ and setting it value at $\nabla img$. This means that $\nabla img$ is a regular non-differentiable gradient of the image, as in the standard method. Assume a function \emph{getSimilarPatches} that extracts patches that are similar to the patch centered at $\mathbf{x}$ in the image. Note that $p_i\in\VV_1$, because $p_i\in\nabla I$. We perform a random walk through the patch with a bias towards the direction of the gradient at the current position  and accumulate the standard edge indicator.
   
   The flow equation now becomes
   \begin{equation}\label{eq:flow2}
     \frac{\partial}{\partial t}C=NonLocal(\nabla I).id\cdot\kappa \vec{N}-\langle NonLocal(\nabla I).d(\&\nabla I),\vec{N}\rangle\vec{N}
     \end{equation}
     where $NonLocal$ computes Algorithm \ref{alg:nonL} for each $\mathbf{x}\in\nabla I$. After replacing the edge indicator function $g$ with the Algorithm \ref{alg:nonL}, we proceed as if performing the standard \emph{GAC}.
     
     Non local methods perform better, as they consider global information about the environment the manifold flows through. Global evaluations are easily formulated by algorithmic control flow, which becomes differentiable through operational calculus and is thus simply employed amongst the many established variational techniques.
         
     
     \section{Conclusions}
     
     Existence of a program is embedded in a virtual reality, forming a system of objects undergoing change in a virtual space. Just as the reality inhabited by us is being studied by science, revealing principles and laws, so can the virtual reality inhabited by programs. Yet here lies a tougher task, as the laws of the system are simultaneously observed and constructed; the universe is bug-free, up to philosophic precision, while our programs are not. This reinforces the need for a language capable of not only capturing, but also constructing digital phenomena, a feat demonstrated by operational calculus. \cite[Section 8]{OperationalCalculus}
  
\clearpage
  \printbibliography
  
\end{document}